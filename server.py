import logging
import os
import time
from concurrent import futures

import grpc

from intellidocspb.v1 import intellidocs_pb2_grpc as intellidocs_pb2_grpc
from intellidocspb.v1 import intellidocs_pb2 as intellidocs_pb2
from model.intellidocs_rag.intellidocs_main import IntellidocsRAG
from utils.constants import ConstantSettings

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class IntelliDocsServiceServicer(intellidocs_pb2_grpc.IntelliDocsServiceServicer):
    """
    Implements the IntelliDocsService service defined in the proto.
    """

    def __init__(self, rag_system):
        """
        Initialize the service with a RAG system.

        Args:
            rag_system: An initialized IntellidocsRAG instance
        """
        self.rag_system = rag_system
        self.document_map = {}

    def load_document(self, document_path):
        """
        Load a document into the RAG system and return its ID.

        Args:
            document_path: Path to the PDF document

        Returns:
            document_id: Unique identifier for the document
        """
        if not os.path.exists(document_path):
            raise FileNotFoundError(f"Document not found: {document_path}")

        # Process the document
        self.rag_system.process([document_path])

        # Get the PDF key generated by the RAG system
        pdf_key = self.rag_system.get_pdf_key(document_path)
        document_id = str(hash(document_path))  # Generate a unique ID
        self.document_map[document_id] = pdf_key

        return document_id

    def ChatWithDocument(self, request_iterator, context):
        """
        Implements the ChatWithDocument RPC method.

        Args:
            request_iterator: Iterator over ChatWithDocumentRequest messages
            context: gRPC context

        Yields:
            ChatWithDocumentResponse messages
        """
        for request in request_iterator:
            logger.info(f"Received query for document ID: {request.document_id}")

            # Check if document is loaded
            if request.document_id not in self.document_map:
                error_msg = f"Document ID {request.document_id} not found. Please load the document first."
                logger.error(error_msg)
                yield intellidocs_pb2.ChatWithDocumentResponse(query_response=[error_msg])
                continue

            pdf_key = self.document_map[request.document_id]

            try:
                # Retrieve relevant chunks using the RAG system
                top_chunks = self.rag_system.retrieve_top_n_custom(
                    user_query=request.user_query,
                    pdf_key=pdf_key,
                    top_n=3  # Retrieve top 3 most relevant chunks
                )

                # Format the response
                if not top_chunks:
                    response = ["No relevant information found in the document."]
                else:
                    # Format each chunk with its relevance score
                    response = [
                        f"Relevance: {chunk['score']:.4f}\nContent: {chunk['chunk']}"
                        for chunk in top_chunks
                    ]

                # Send the response
                yield intellidocs_pb2.ChatWithDocumentResponse(query_response=response)

            except Exception as e:
                error_msg = f"Error processing query: {str(e)}"
                logger.error(error_msg)
                yield intellidocs_pb2.ChatWithDocumentResponse(query_response=[error_msg])


def serve(port=50051, chunk_size=2000, embedding_model="all-MiniLM-L6-v2", chroma_db_dir="./chroma_db"):
    """
    Start the gRPC server.

    Args:
        port: Port number to listen on
        chunk_size: Size of text chunks for the RAG system
        embedding_model: Name of the embedding model to use
        chroma_db_dir: Directory to store the vector database
    """
    # Initialize the RAG system
    rag_system = IntellidocsRAG(
        chunk_size=chunk_size,
        embedding_model=embedding_model,
        chroma_db_dir=chroma_db_dir
    )

    # Create a gRPC server
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))

    # Add the service to the server
    intellidocs_pb2_grpc.add_IntelliDocsServiceServicer_to_server(
        IntelliDocsServiceServicer(rag_system), server
    )

    # Start the server
    server.add_insecure_port(f'[::]:{port}')
    server.start()
    logger.info(f"Server started, listening on port {port}")

    # Keep the server running
    try:
        while True:
            time.sleep(86400)  # Sleep for a day
    except KeyboardInterrupt:
        server.stop(0)
        logger.info("Server stopped")


if __name__ == "__main__":
    # Parse command line arguments
    import argparse

    parser = argparse.ArgumentParser(description="IntelliDocs gRPC Server")
    parser.add_argument("--port", type=int, default=ConstantSettings.gRPC_PORT, help="Port to listen on")
    parser.add_argument("--chunk-size", type=int, default=ConstantSettings.CHUNK_SIZE, help="Text chunk size")
    parser.add_argument("--embedding-model", type=str, default=ConstantSettings.EMBEDDING_MODEL_NAME,
                        help="Embedding model name")
    parser.add_argument("--chroma-db-dir", type=str, default="./chroma_db",
                        help="ChromaDB directory")

    args = parser.parse_args()

    # Start the server
    serve(
        port=args.port,
        chunk_size=args.chunk_size,
        embedding_model=args.embedding_model,
        chroma_db_dir=args.chroma_db_dir
    )
